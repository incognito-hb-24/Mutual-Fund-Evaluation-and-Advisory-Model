{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231d087-0eeb-4d37-b1a6-c4ac51a41749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, json, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASE = \"/content/drive/MyDrive/MLBA_Project\"\n",
    "PRO  = f\"{BASE}/data/processed\"\n",
    "RPT  = f\"{BASE}/reports\"\n",
    "for d in [PRO, RPT]: os.makedirs(d, exist_ok=True)\n",
    "\n",
    "oof = pd.read_csv(f\"{PRO}/clf_oof_predictions.csv\", parse_dates=[\"date\"])\n",
    "oof.columns = oof.columns.str.lower()\n",
    "\n",
    "score_col = \"ens_weighted\" if \"ens_weighted\" in oof.columns else \"ens_tab\"\n",
    "assert set([\"date\",\"fund_id\",\"y_true\"]).issubset(oof.columns), \"OOF missing key columns\"\n",
    "assert score_col in oof.columns, f\"Missing ensemble column: {score_col}\"\n",
    "\n",
    "if \"y_excess_63d\" not in oof.columns:\n",
    "    mst = pd.read_csv(f\"{PRO}/clean_master_union.csv\", parse_dates=[\"date\"])\n",
    "    mst.columns = mst.columns.str.lower()\n",
    "    mst = mst.sort_values([\"fund_id\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "    H = 63\n",
    "    mst[\"fwd_ret_63d_fund\"] = mst.groupby(\"fund_id\")[\"nav\"].shift(-H) / mst[\"nav\"] - 1.0\n",
    "    tri_daily = mst[[\"date\",\"tri\"]].drop_duplicates().sort_values(\"date\").copy()\n",
    "    tri_daily[\"tri_fwd\"] = tri_daily[\"tri\"].shift(-H)\n",
    "    tri_daily[\"fwd_ret_63d_bmk\"] = tri_daily[\"tri_fwd\"] / tri_daily[\"tri\"] - 1.0\n",
    "    mst = mst.merge(tri_daily[[\"date\",\"fwd_ret_63d_bmk\"]], on=\"date\", how=\"left\")\n",
    "    mst[\"y_excess_63d\"] = mst[\"fwd_ret_63d_fund\"] - mst[\"fwd_ret_63d_bmk\"]\n",
    "\n",
    "    oof = oof.merge(mst[[\"date\",\"fund_id\",\"y_excess_63d\"]], on=[\"date\",\"fund_id\"], how=\"left\")\n",
    "\n",
    "feat = pd.read_csv(f\"{PRO}/clean_with_features.csv\", parse_dates=[\"date\"])\n",
    "feat.columns = feat.columns.str.lower()\n",
    "\n",
    "y_true = oof[\"y_true\"].values\n",
    "scores = oof[score_col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ef036-8590-488b-a69e-0fb7ceca4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def bootstrap_ci_prauc(y, s, B=1000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y, s = np.asarray(y), np.asarray(s)\n",
    "    m = (~np.isnan(y)) & (~np.isnan(s))\n",
    "    y, s = y[m].astype(int), s[m].astype(float)\n",
    "    if len(y) == 0: return np.nan, np.nan, np.nan\n",
    "    idxs = rng.integers(0, len(y), size=(B, len(y)))\n",
    "    vals = [average_precision_score(y[i], s[i]) for i in idxs]\n",
    "    mean = float(np.mean(vals)); lo, hi = np.percentile(vals, [2.5, 97.5])\n",
    "    return mean, float(lo), float(hi)\n",
    "\n",
    "rows = []\n",
    "for name in [\"logit\",\"xgb\",\"lgb\",score_col]:\n",
    "    if name not in oof.columns: continue\n",
    "    m, lo, hi = bootstrap_ci_prauc(oof[\"y_true\"], oof[name])\n",
    "    label = \"Ensemble(Weighted)\" if name == \"ens_weighted\" else (\"Ensemble\" if name == \"ens_tab\" else name.capitalize())\n",
    "    rows.append({\"model\": label, \"prauc_mean\": m, \"prauc_lo95\": lo, \"prauc_hi95\": hi})\n",
    "\n",
    "ci_df = pd.DataFrame(rows)\n",
    "ci_path = f\"{RPT}/pr_auc_ci.csv\"\n",
    "ci_df.to_csv(ci_path, index=False)\n",
    "print(\"Saved →\", ci_path)\n",
    "ci_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509a156-52f3-4557-b45c-d78218b49dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def threshold_sweep_with_cost(y, s, realized_excess=None, tx=0.002):\n",
    "    y, s = np.asarray(y), np.asarray(s)\n",
    "    m = (~np.isnan(y)) & (~np.isnan(s))\n",
    "    yv, sv = y[m].astype(int), s[m].astype(float)\n",
    "    prec, rec, th = precision_recall_curve(yv, sv)\n",
    "    sweep = pd.DataFrame({\"threshold\": list(th) + [np.inf], \"precision\": prec, \"recall\": rec})\n",
    "\n",
    "    re_all = None\n",
    "    if realized_excess is not None:\n",
    "        r = np.asarray(realized_excess)\n",
    "        re_all = np.where(m, np.nan_to_num(r, nan=0.0), np.nan)\n",
    "\n",
    "    def cost_at(thr):\n",
    "        pred = (sv >= thr).astype(int)\n",
    "        if re_all is not None:\n",
    "            re = re_all[~np.isnan(re_all)][0:len(sv)]\n",
    "            tp = (pred==1) & (yv==1)\n",
    "            fp = (pred==1) & (yv==0)\n",
    "            tp_net = re[tp].sum() - tx * tp.sum()\n",
    "            fp_net = re[fp].sum() - tx * fp.sum()\n",
    "            return float(tp_net + fp_net)\n",
    "        else:\n",
    "            return float(((pred==1)&(yv==1)).sum()*0.05 - ((pred==1)&(yv==0)).sum()*0.02 - tx*(pred==1).sum())\n",
    "\n",
    "    sweep[\"cost\"] = sweep[\"threshold\"].apply(cost_at)\n",
    "    return sweep\n",
    "\n",
    "ens_sweep = threshold_sweep_with_cost(\n",
    "    oof[\"y_true\"].values,\n",
    "    oof[score_col].values,\n",
    "    realized_excess=oof[\"y_excess_63d\"].values if \"y_excess_63d\" in oof.columns else None\n",
    ")\n",
    "ens_sweep_path = f\"{RPT}/threshold_sweep_{score_col}.csv\"\n",
    "ens_sweep.to_csv(ens_sweep_path, index=False)\n",
    "print(\"Saved →\", ens_sweep_path)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "x = ens_sweep[\"threshold\"].replace(np.inf, np.nan)\n",
    "plt.plot(x, ens_sweep[\"cost\"])\n",
    "plt.xlabel(\"Threshold\"); plt.ylabel(\"Net payoff (arb. units)\")\n",
    "plt.title(f\"Cost Curve — {('Weighted Ensemble' if score_col=='ens_weighted' else 'Ensemble')}\")\n",
    "plt.grid(True, alpha=0.3); plt.tight_layout()\n",
    "plt.savefig(f\"{RPT}/cost_curve_{score_col}.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "ens_sweep.sort_values(\"cost\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e5c17e-30ae-408b-b306-8a3d971ba60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lift_and_topdecile(df, date_col, label_col, score_col, tag):\n",
    "    m = df[[date_col, label_col, score_col]].dropna().copy()\n",
    "    if m.empty:\n",
    "        print(f\"[{tag}] No valid rows.\"); return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    m[\"score_q\"] = m[score_col].rank(pct=True)\n",
    "    m[\"bucket\"] = (m[\"score_q\"] * 10).astype(int).clip(0, 9)\n",
    "\n",
    "    lift = (m.groupby(\"bucket\", as_index=False)[label_col]\n",
    "              .mean()\n",
    "              .rename(columns={label_col: \"positive_rate\"}))\n",
    "    lift_path = f\"{RPT}/lift_by_bucket_{tag}.csv\"\n",
    "    lift.to_csv(lift_path, index=False)\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(lift[\"bucket\"], lift[\"positive_rate\"], marker=\"o\")\n",
    "    plt.xticks(range(0,10)); plt.xlabel(\"Score Decile (0=low, 9=high)\")\n",
    "    plt.ylabel(\"Positive Rate\"); plt.title(f\"Lift by Decile — {tag}\")\n",
    "    plt.grid(True, alpha=0.3); plt.tight_layout()\n",
    "    lift_png = f\"{RPT}/lift_by_bucket_{tag}.png\"\n",
    "    plt.savefig(lift_png, dpi=160); plt.show()\n",
    "    print(f\"[{tag}] Saved → {lift_path}, {lift_png}\")\n",
    "\n",
    "    m[\"year\"] = pd.to_datetime(m[date_col]).dt.year\n",
    "    by_year = (m[m[\"score_q\"]>=0.90].groupby(\"year\", as_index=False)[label_col]\n",
    "                 .mean()\n",
    "                 .rename(columns={label_col: \"precision_top10\"}))\n",
    "    by_year_path = f\"{RPT}/precision_by_year_top10_{tag}.csv\"\n",
    "    by_year.to_csv(by_year_path, index=False)\n",
    "    print(f\"[{tag}] Saved → {by_year_path}\")\n",
    "    return lift, by_year\n",
    "\n",
    "tag = \"ensemble_weighted\" if score_col==\"ens_weighted\" else \"ensemble\"\n",
    "ens_lift, ens_by_year = lift_and_topdecile(\n",
    "    df=oof.rename(columns={score_col:\"score\"}),\n",
    "    date_col=\"date\", label_col=\"y_true\", score_col=\"score\", tag=tag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9b3ad-c94e-4145-8ef3-1a3422bb856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, average_precision_score\n",
    "\n",
    "def balanced_business_threshold(y_true, scores, target_precision=0.70, min_recall=0.30):\n",
    "    y = np.asarray(y_true, dtype=float); s = np.asarray(scores, dtype=float)\n",
    "    m = (~np.isnan(y)) & (~np.isnan(s))\n",
    "    if m.sum()==0: return 0.5, np.nan, np.nan\n",
    "    prec, rec, th = precision_recall_curve(y[m].astype(int), s[m])\n",
    "    for i in range(len(th)):\n",
    "        if prec[i] >= target_precision and rec[i] >= min_recall:\n",
    "            return th[i], float(prec[i]), float(rec[i])\n",
    "    f1 = 2*prec*rec/(prec+rec+1e-12)\n",
    "    idx = np.nanargmax(f1[:-1]); return th[idx], float(prec[idx]), float(rec[idx])\n",
    "\n",
    "def dump_fp_fn(tag, df_scored, score_col, label_col=\"y_true\", top_k=25):\n",
    "    scores = df_scored[score_col].values\n",
    "    labels = df_scored[label_col].values\n",
    "    th_star, P, R = balanced_business_threshold(labels, scores, 0.70, 0.30)\n",
    "    pred = (scores >= th_star).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, pred).ravel()\n",
    "    pr_auc = average_precision_score(labels[~np.isnan(scores)], scores[~np.isnan(scores)])\n",
    "\n",
    "    out = df_scored.copy()\n",
    "    out[\"pred\"] = pred\n",
    "    out[\"margin\"] = np.where(out[\"pred\"]==1, out[score_col], 1.0 - out[score_col])\n",
    "\n",
    "    FP = out[(out[\"pred\"]==1) & (out[label_col]==0)].sort_values(\"margin\", ascending=False).head(top_k)\n",
    "    FN = out[(out[\"pred\"]==0) & (out[label_col]==1)].sort_values(\"margin\", ascending=False).head(top_k)\n",
    "\n",
    "    FP.to_csv(f\"{RPT}/representative_fp_{tag}.csv\", index=False)\n",
    "    FN.to_csv(f\"{RPT}/representative_fn_{tag}.csv\", index=False)\n",
    "\n",
    "    cm_df = pd.DataFrame([[tn, fp],[fn, tp]], columns=[\"Pred 0\",\"Pred 1\"], index=[\"Actual 0\",\"Actual 1\"])\n",
    "    cm_df.to_csv(f\"{RPT}/cm_{tag}.csv\")\n",
    "\n",
    "    print(f\"\\n[{tag.upper()}] PR-AUC={pr_auc:.4f} | thr={th_star:.3f} | P={P:.3f} R={R:.3f}\")\n",
    "    print(cm_df)\n",
    "\n",
    "base = oof[[\"date\",\"fund_id\",\"y_true\",score_col]].dropna(subset=[\"y_true\",score_col]).copy()\n",
    "dump_fp_fn(tag, base.rename(columns={score_col:\"score\"}), score_col=\"score\", label_col=\"y_true\", top_k=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9c970-9ce6-41cd-9896-02b300a057ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "dfm = feat.merge(oof[[\"date\",\"fund_id\",\"y_true\"]], on=[\"date\",\"fund_id\"], how=\"inner\").copy()\n",
    "dfm = dfm.sort_values([\"fund_id\",\"date\"]).reset_index(drop=True)\n",
    "ban = {\"date\",\"fund_id\",\"nav\",\"tri\",\"y_true\",\"y_excess_63d\"}\n",
    "feat_cols_all = [c for c in dfm.columns if (c not in ban) and pd.api.types.is_numeric_dtype(dfm[c])]\n",
    "\n",
    "mask = dfm[feat_cols_all + [\"y_true\"]].notna().all(axis=1)\n",
    "X0 = dfm.loc[mask, feat_cols_all].astype(float)\n",
    "y0 = dfm.loc[mask, \"y_true\"].astype(int)\n",
    "\n",
    "def run_lgb(X, y, n_splits=3, seed=42):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    ap = []\n",
    "    for tr, te in tscv.split(X):\n",
    "        m = LGBMClassifier(\n",
    "            n_estimators=600, learning_rate=0.05, num_leaves=31,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "            random_state=seed, n_jobs=-1\n",
    "        )\n",
    "        m.fit(X.iloc[tr], y.iloc[tr])\n",
    "        s = m.predict_proba(X.iloc[te])[:,1]\n",
    "        ap.append(average_precision_score(y.iloc[te], s))\n",
    "    return float(np.mean(ap)) if len(ap) else np.nan\n",
    "\n",
    "rows_abl = []\n",
    "pr_base = run_lgb(X0, y0) if len(X0) and len(np.unique(y0))>1 else np.nan\n",
    "rows_abl.append({\"ablation\":\"baseline_all\", \"features\":len(feat_cols_all), \"PR_AUC\": pr_base})\n",
    "\n",
    "macro_keys = [\"india_vix\",\"usd_inr\",\"gsec_10y\",\"gold_inr\",\"brent_usd\"]\n",
    "feat_nomacro = [c for c in feat_cols_all if not any(k in c for k in macro_keys)]\n",
    "pr_nomacro = run_lgb(dfm.loc[mask, feat_nomacro].astype(float), y0) if len(feat_nomacro) else np.nan\n",
    "rows_abl.append({\"ablation\":\"no_macro\", \"features\":len(feat_nomacro), \"PR_AUC\": pr_nomacro})\n",
    "\n",
    "def alt_window_run(include_keys):\n",
    "    include_keys = [k for k in include_keys if any(k in c for c in feat_cols_all)]\n",
    "    if not include_keys: return np.nan, 0\n",
    "    subset = [c for c in feat_cols_all if not any(k in c for k in [\"ret_21d\",\"excess_21d\",\"ret_63d\",\"excess_63d\"])]\n",
    "    subset += [c for c in feat_cols_all if any(k in c for k in include_keys)]\n",
    "    subset = sorted(set(subset))\n",
    "    Xm = dfm.loc[mask, subset].astype(float)\n",
    "    return run_lgb(Xm, y0), len(subset)\n",
    "\n",
    "for tag_alt, keys in [(\"windows_7d\", [\"ret_7d\",\"excess_7d\"]),\n",
    "                      (\"windows_14d\",[\"ret_14d\",\"excess_14d\"])]:\n",
    "    pr_alt, nfeat = alt_window_run(keys)\n",
    "    rows_abl.append({\"ablation\":tag_alt, \"features\":nfeat, \"PR_AUC\": pr_alt})\n",
    "\n",
    "abl_df = pd.DataFrame(rows_abl)\n",
    "abl_path = f\"{RPT}/ablation_window_macro.csv\"\n",
    "abl_df.to_csv(abl_path, index=False)\n",
    "print(\"Saved →\", abl_path)\n",
    "abl_df.round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
