{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb82380-5ced-49d8-9e4e-3b59631fd36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, numpy as np, pandas as pd\n",
    "np.random.seed(42)\n",
    "\n",
    "BASE = \"/content/drive/MyDrive/MLBA_Project\"\n",
    "RAW  = f\"{BASE}/data/raw\"\n",
    "PRO  = f\"{BASE}/data/processed\"\n",
    "MDIR = f\"{BASE}/models\"\n",
    "RPT  = f\"{BASE}/reports\"\n",
    "for d in (PRO, MDIR, RPT): os.makedirs(d, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36167fd7-5ea9-48ac-b342-7a90d17c7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pd.read_csv(f\"{PRO}/clean_with_features.csv\", parse_dates=[\"date\"])\n",
    "feat.columns = feat.columns.str.lower()\n",
    "\n",
    "mst = pd.read_csv(f\"{PRO}/clean_master_union.csv\", parse_dates=[\"date\"])\n",
    "mst.columns = mst.columns.str.lower()\n",
    "mst = mst.sort_values([\"fund_id\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "H = 63\n",
    "nav_fwd = mst.groupby(\"fund_id\")[\"nav\"].shift(-H)\n",
    "mst[\"fwd_ret_63d_fund\"] = nav_fwd / mst[\"nav\"] - 1.0\n",
    "\n",
    "tri_daily = mst[[\"date\",\"tri\"]].drop_duplicates().sort_values(\"date\")\n",
    "tri_daily[\"tri_fwd\"] = tri_daily[\"tri\"].shift(-H)\n",
    "tri_daily[\"fwd_ret_63d_bmk\"] = tri_daily[\"tri_fwd\"] / tri_daily[\"tri\"] - 1.0\n",
    "\n",
    "mst = mst.merge(tri_daily[[\"date\",\"fwd_ret_63d_bmk\"]], on=\"date\", how=\"left\")\n",
    "mst[\"y_excess_63d\"] = mst[\"fwd_ret_63d_fund\"] - mst[\"fwd_ret_63d_bmk\"]\n",
    "\n",
    "lab = mst[[\"date\",\"fund_id\",\"y_excess_63d\"]].dropna().copy()\n",
    "dfm = feat.merge(lab, on=[\"date\",\"fund_id\"], how=\"inner\").sort_values([\"fund_id\",\"date\"]).reset_index(drop=True)\n",
    "dfm[\"y_bin\"] = (dfm[\"y_excess_63d\"] > 0).astype(int)\n",
    "\n",
    "print(\"Rows:\", len(dfm), \"| Funds:\", dfm.fund_id.nunique(),\n",
    "      \"| Range:\", dfm.date.min().date(), \"→\", dfm.date.max().date(),\n",
    "      \"| Pos rate:\", round(dfm[\"y_bin\"].mean(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541bc27-fc4c-4e23-bec3-1abecb089594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "ban = {\"date\",\"fund_id\",\"nav\",\"tri\",\"y_excess_63d\",\"y_bin\"}\n",
    "feat_cols = [c for c in dfm.columns if (c not in ban) and pd.api.types.is_numeric_dtype(dfm[c])]\n",
    "\n",
    "mask = dfm[feat_cols + [\"y_bin\"]].notna().all(axis=1)\n",
    "dfm = dfm.loc[mask].reset_index(drop=True)\n",
    "\n",
    "X = dfm[feat_cols].astype(float)\n",
    "y = dfm[\"y_bin\"].astype(int)\n",
    "meta_idx = dfm[[\"date\",\"fund_id\"]].reset_index(drop=True)\n",
    "\n",
    "print(\"Features:\", len(feat_cols), \"| Rows:\", len(X))\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "fold_rows = []\n",
    "for k, (tr, te) in enumerate(tscv.split(X), 1):\n",
    "    dtr = meta_idx.iloc[tr][\"date\"]; dte = meta_idx.iloc[te][\"date\"]\n",
    "    fold_rows.append({\n",
    "        \"fold\": k,\n",
    "        \"train_start\": str(dtr.min().date()), \"train_end\": str(dtr.max().date()),\n",
    "        \"test_start\":  str(dte.min().date()), \"test_end\":  str(dte.max().date()),\n",
    "        \"train_n\": len(tr), \"test_n\": len(te)\n",
    "    })\n",
    "split_table = pd.DataFrame(fold_rows)\n",
    "split_table.to_csv(f\"{PRO}/split_table_classification.csv\", index=False)\n",
    "split_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c211576-5514-4476-a211-1056deee74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, precision_recall_curve, confusion_matrix,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "def save_fold_preds(model_name, fold_id, test_idx, scores, y_true_fold, out_dir=PRO):\n",
    "    out = meta_idx.iloc[test_idx].copy()\n",
    "    out[\"y_true\"] = y_true_fold\n",
    "    out[model_name] = scores\n",
    "    out[\"fold\"] = fold_id\n",
    "    out.to_csv(f\"{out_dir}/clf_fold_{model_name}_{fold_id}.csv\", index=False)\n",
    "\n",
    "def pr_auc_and_sweep(y_true, scores):\n",
    "    pr_auc = average_precision_score(y_true, scores)\n",
    "    prec, rec, th = precision_recall_curve(y_true, scores)\n",
    "    sweep = pd.DataFrame({\"threshold\": list(th) + [np.inf], \"precision\": prec, \"recall\": rec})\n",
    "    return pr_auc, sweep\n",
    "\n",
    "def business_aware_threshold(y_true, scores, min_precision=0.65):\n",
    "    prec, rec, th = precision_recall_curve(y_true, scores)\n",
    "    for i in range(len(th)):\n",
    "        if prec[i] >= min_precision and rec[i] >= 0.30:\n",
    "            f1 = 2 * prec[i] * rec[i] / (prec[i] + rec[i] + 1e-12)\n",
    "            return th[i], float(prec[i]), float(rec[i]), float(f1)\n",
    "    f1_all = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "    idx = int(np.nanargmax(f1_all[:-1])) if len(f1_all) > 1 else 0\n",
    "    th_fb = th[idx] if idx < len(th) else 0.5\n",
    "    pred_fb = (scores >= th_fb).astype(int)\n",
    "    return (float(th_fb),\n",
    "            float(precision_score(y_true, pred_fb, zero_division=0)),\n",
    "            float(recall_score(y_true, pred_fb, zero_division=0)),\n",
    "            float(f1_score(y_true, pred_fb, zero_division=0)))\n",
    "\n",
    "def cm_at_threshold(y_true, scores, th):\n",
    "    yhat = (scores >= th).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, yhat).ravel()\n",
    "    return tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14379852-d01a-490e-97fc-869d30ab28fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "logit = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    LogisticRegression(max_iter=500, class_weight=\"balanced\", random_state=42)\n",
    ")\n",
    "\n",
    "oof_logit = np.full(len(y), np.nan, dtype=float)\n",
    "cv_rows = []\n",
    "\n",
    "for k, (tr, te) in enumerate(tscv.split(X), 1):\n",
    "    logit.fit(X.iloc[tr], y.iloc[tr])\n",
    "    s = logit.predict_proba(X.iloc[te])[:,1]\n",
    "    oof_logit[te] = s\n",
    "    pr_auc, _ = pr_auc_and_sweep(y.iloc[te], s)\n",
    "    cv_rows.append({\"fold\": k, \"PR_AUC\": pr_auc, \"test_n\": len(te)})\n",
    "    save_fold_preds(\"logit\", k, te, s, y.iloc[te].values)\n",
    "\n",
    "cv_logit = pd.DataFrame(cv_rows)\n",
    "cv_logit.to_csv(f\"{PRO}/cv_logistic_folds.csv\", index=False)\n",
    "print(\"Logit mean PR-AUC:\", round(cv_logit[\"PR_AUC\"].mean(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10787779-7e5b-4348-a89e-88767e4bb495",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=700, max_depth=4, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "    random_state=42, n_jobs=-1, eval_metric=\"logloss\",\n",
    ")\n",
    "\n",
    "oof_xgb = np.full(len(y), np.nan, dtype=float)\n",
    "cv_rows = []\n",
    "\n",
    "for k, (tr, te) in enumerate(tscv.split(X), 1):\n",
    "    xgb.fit(X.iloc[tr], y.iloc[tr])\n",
    "    s = xgb.predict_proba(X.iloc[te])[:,1]\n",
    "    oof_xgb[te] = s\n",
    "    pr_auc, _ = pr_auc_and_sweep(y.iloc[te], s)\n",
    "    cv_rows.append({\"fold\": k, \"PR_AUC\": pr_auc, \"test_n\": len(te)})\n",
    "    save_fold_preds(\"xgb\", k, te, s, y.iloc[te].values)\n",
    "\n",
    "cv_xgb = pd.DataFrame(cv_rows)\n",
    "cv_xgb.to_csv(f\"{PRO}/cv_xgb_folds.csv\", index=False)\n",
    "print(\"XGB mean PR-AUC:\", round(cv_xgb[\"PR_AUC\"].mean(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f41cb-ee22-4ade-a9dc-ff775dab1308",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb = LGBMClassifier(\n",
    "    n_estimators=1200, learning_rate=0.03,\n",
    "    num_leaves=31, subsample=0.8, colsample_bytree=0.8,\n",
    "    reg_lambda=1.0, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "oof_lgb = np.full(len(y), np.nan, dtype=float)\n",
    "cv_rows = []\n",
    "\n",
    "for k, (tr, te) in enumerate(tscv.split(X), 1):\n",
    "    lgb.fit(X.iloc[tr], y.iloc[tr])\n",
    "    s = lgb.predict_proba(X.iloc[te])[:,1]\n",
    "    oof_lgb[te] = s\n",
    "    pr_auc, _ = pr_auc_and_sweep(y.iloc[te], s)\n",
    "    cv_rows.append({\"fold\": k, \"PR_AUC\": pr_auc, \"test_n\": len(te)})\n",
    "    save_fold_preds(\"lgb\", k, te, s, y.iloc[te].values)\n",
    "\n",
    "cv_lgb = pd.DataFrame(cv_rows)\n",
    "cv_lgb.to_csv(f\"{PRO}/cv_lgb_folds.csv\", index=False)\n",
    "print(\"LGB mean PR-AUC:\", round(cv_lgb[\"PR_AUC\"].mean(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa9640-fef4-41c2-85cc-1076fd361b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = meta_idx.copy()\n",
    "oof[\"y_true\"] = y.values\n",
    "oof[\"logit\"]  = oof_logit\n",
    "oof[\"xgb\"]    = oof_xgb\n",
    "oof[\"lgb\"]    = oof_lgb\n",
    "\n",
    "fold_id = np.full(len(oof), np.nan)\n",
    "for name in [\"logit\",\"xgb\",\"lgb\"]:\n",
    "    for k in range(1, 6):\n",
    "        dfk = pd.read_csv(f\"{PRO}/clf_fold_{name}_{k}.csv\", parse_dates=[\"date\"])\n",
    "        key = oof[[\"date\",\"fund_id\"]].merge(\n",
    "            dfk[[\"date\",\"fund_id\"]].assign(fold=k),\n",
    "            on=[\"date\",\"fund_id\"], how=\"left\"\n",
    "        )[\"fold\"].values\n",
    "        mask = np.isnan(fold_id) & ~np.isnan(key)\n",
    "        fold_id[mask] = key[mask]\n",
    "oof[\"fold\"] = fold_id\n",
    "\n",
    "oof[\"ens_tab\"] = np.nanmean(oof[[\"logit\",\"xgb\",\"lgb\"]].values, axis=1)\n",
    "\n",
    "oof_path = f\"{PRO}/clf_oof_predictions.csv\"\n",
    "oof.to_csv(oof_path, index=False)\n",
    "print(\"Saved:\", oof_path)\n",
    "\n",
    "valid_mask = oof[[\"y_true\",\"ens_tab\"]].notna().all(axis=1)\n",
    "if valid_mask.sum() == 0:\n",
    "    th_best = 0.30\n",
    "    print(\"No valid rows; using fallback th_best=0.30\")\n",
    "else:\n",
    "    yv, sv = oof.loc[valid_mask, \"y_true\"].values, oof.loc[valid_mask, \"ens_tab\"].values\n",
    "    th_best, P, R, F1 = business_aware_threshold(yv, sv, min_precision=0.7)\n",
    "    print(f\"Chosen th_best={th_best:.4f} | P={P:.3f} R={R:.3f} F1={F1:.3f}\")\n",
    "\n",
    "ens_rows = []\n",
    "for k in range(1, 6):\n",
    "    m = (oof[\"fold\"] == k) & ~np.isnan(oof[\"ens_tab\"])\n",
    "    if m.sum():\n",
    "        pr_auc, _ = pr_auc_and_sweep(oof.loc[m, \"y_true\"].values, oof.loc[m, \"ens_tab\"].values)\n",
    "        ens_rows.append({\"fold\": k, \"PR_AUC\": pr_auc, \"test_n\": int(m.sum())})\n",
    "\n",
    "cv_ens = pd.DataFrame(ens_rows)\n",
    "cv_ens.to_csv(f\"{PRO}/cv_ensemble_folds.csv\", index=False)\n",
    "print(\"Ensemble mean PR-AUC:\", round(cv_ens[\"PR_AUC\"].mean(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b01727c-2644-4768-96dd-2ae5827dae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, json, numpy as np, pandas as pd\n",
    "\n",
    "logit.fit(X, y)\n",
    "xgb.fit(X, y)\n",
    "lgb.fit(X, y)\n",
    "\n",
    "as_of = meta_idx[\"date\"].max()\n",
    "mask_last = (meta_idx[\"date\"] == as_of).values\n",
    "X_last = X.loc[mask_last]\n",
    "idx_last = meta_idx.loc[mask_last].reset_index(drop=True)\n",
    "\n",
    "s_logit = logit.predict_proba(X_last)[:,1]\n",
    "s_xgb   = xgb.predict_proba(X_last)[:,1]\n",
    "s_lgb   = lgb.predict_proba(X_last)[:,1]\n",
    "s_ens   = np.nanmean(np.c_[s_logit, s_xgb, s_lgb], axis=1)\n",
    "\n",
    "pred = idx_last.copy()\n",
    "pred[\"score_logit\"] = s_logit\n",
    "pred[\"score_xgb\"]   = s_xgb\n",
    "pred[\"score_lgb\"]   = s_lgb\n",
    "pred[\"score_ens\"]   = s_ens\n",
    "\n",
    "def to_5class(prob, base):\n",
    "    if prob >= base + 0.20: return \"Strong Buy\"\n",
    "    if prob >= base + 0.10: return \"Buy\"\n",
    "    if base - 0.10 <= prob < base + 0.10: return \"Neutral\"\n",
    "    if base - 0.20 <= prob < base - 0.10: return \"Sell\"\n",
    "    return \"Strong Sell\"\n",
    "\n",
    "pred[\"signal\"] = pred[\"score_ens\"].apply(lambda p: to_5class(p, th_best))\n",
    "pred.to_csv(f\"{PRO}/leaderboard_model.csv\", index=False)\n",
    "pred.to_csv(f\"{PRO}/model_predictions.csv\", index=False)\n",
    "print(\"Saved latest predictions →\", f\"{PRO}/leaderboard_model.csv\")\n",
    "\n",
    "joblib.dump(logit, f\"{MDIR}/logit.pkl\")\n",
    "joblib.dump(xgb,   f\"{MDIR}/xgb.pkl\")\n",
    "joblib.dump(lgb,   f\"{MDIR}/lgb.pkl\")\n",
    "\n",
    "with open(f\"{MDIR}/meta.json\",\"w\") as f:\n",
    "    json.dump({\n",
    "        \"seed\": 42,\n",
    "        \"horizon_days\": int(H),\n",
    "        \"features\": feat_cols,\n",
    "        \"cv_splits\": 5,\n",
    "        \"as_of\": str(as_of.date()),\n",
    "        \"business_threshold\": float(th_best)\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Saved models and meta.\")\n",
    "print(\"Signal distribution on latest date:\")\n",
    "print(pred[\"signal\"].value_counts())\n",
    "pred.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd57675-bd97-4fc3-8d70-c5e6c0370aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "reg_mask = dfm[feat_cols + [\"y_excess_63d\"]].notna().all(axis=1)\n",
    "Xr = dfm.loc[reg_mask, feat_cols].astype(float)\n",
    "yr = dfm.loc[reg_mask, \"y_excess_63d\"].astype(float).values\n",
    "idx_r = dfm.loc[reg_mask, [\"date\",\"fund_id\"]].reset_index(drop=True)\n",
    "\n",
    "lgb_m = LGBMRegressor(objective=\"quantile\", alpha=0.5, n_estimators=800, learning_rate=0.05,\n",
    "                      num_leaves=31, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1)\n",
    "lgb_l = LGBMRegressor(objective=\"quantile\", alpha=0.1, n_estimators=800, learning_rate=0.05,\n",
    "                      num_leaves=31, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1)\n",
    "lgb_u = LGBMRegressor(objective=\"quantile\", alpha=0.9, n_estimators=800, learning_rate=0.05,\n",
    "                      num_leaves=31, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1)\n",
    "\n",
    "lgb_m.fit(Xr, yr); lgb_l.fit(Xr, yr); lgb_u.fit(Xr, yr)\n",
    "\n",
    "as_of = idx_r[\"date\"].max()\n",
    "ix = (idx_r[\"date\"] == as_of).values\n",
    "Xr_last = Xr.loc[ix]\n",
    "idx_last_r = idx_r.loc[ix].reset_index(drop=True)\n",
    "\n",
    "p50 = lgb_m.predict(Xr_last)\n",
    "p10 = lgb_l.predict(Xr_last)\n",
    "p90 = lgb_u.predict(Xr_last)\n",
    "\n",
    "pi = idx_last_r.copy()\n",
    "pi[\"pred_med\"]    = p50\n",
    "pi[\"pred_lo_p10\"] = p10\n",
    "pi[\"pred_hi_p90\"] = p90\n",
    "\n",
    "def decide(lo, hi):\n",
    "    if lo > 0: return \"Buy\"\n",
    "    if hi < 0: return \"Sell\"\n",
    "    return \"Hold\"\n",
    "\n",
    "pi[\"policy\"] = [decide(a,b) for a,b in zip(pi[\"pred_lo_p10\"], pi[\"pred_hi_p90\"])]\n",
    "pi.to_csv(f\"{PRO}/policy_simulation.csv\", index=False)\n",
    "pi.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b315f-285d-496b-a252-0ec2803e90cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def smape(y_true, y_pred, eps=1e-9):\n",
    "    y_true, y_pred = np.asarray(y_true, float), np.asarray(y_pred, float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)).clip(min=eps)\n",
    "    return float(np.mean(2.0 * np.abs(y_pred - y_true) / denom))\n",
    "\n",
    "def bootstrap_ci(metric_fn, y, yhat, B=1000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y = np.asarray(y, float); yhat = np.asarray(yhat, float)\n",
    "    n = len(y); stats = []\n",
    "    for _ in range(B):\n",
    "        idx = rng.integers(0, n, n)\n",
    "        stats.append(metric_fn(y[idx], yhat[idx]))\n",
    "    m = float(np.mean(stats)); lo, hi = np.percentile(stats, [2.5, 97.5])\n",
    "    return m, float(lo), float(hi)\n",
    "\n",
    "dfm = dfm.sort_values([\"fund_id\",\"date\"]).reset_index(drop=True)\n",
    "dfm[\"y_naive_63\"] = dfm.groupby(\"fund_id\")[\"y_excess_63d\"].shift(63)\n",
    "reg_mask = dfm[feat_cols + [\"y_excess_63d\"]].notna().all(axis=1)\n",
    "Xr = dfm.loc[reg_mask, feat_cols].astype(float)\n",
    "yr = dfm.loc[reg_mask, \"y_excess_63d\"].astype(float).values\n",
    "y_naive_all = dfm.loc[reg_mask, \"y_naive_63\"].reset_index(drop=True).values\n",
    "idx_r = dfm.loc[reg_mask, [\"date\",\"fund_id\"]].reset_index(drop=True)\n",
    "\n",
    "tscv_reg = TimeSeriesSplit(n_splits=5)\n",
    "oof_p50 = np.full(len(Xr), np.nan, dtype=float)\n",
    "\n",
    "for k, (tr, te) in enumerate(tscv_reg.split(Xr), 1):\n",
    "    m50 = LGBMRegressor(objective=\"quantile\", alpha=0.5, n_estimators=800, learning_rate=0.05,\n",
    "                        num_leaves=31, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1)\n",
    "    m50.fit(Xr.iloc[tr], yr[tr])\n",
    "    pred_te = m50.predict(Xr.iloc[te])\n",
    "    oof_p50[te] = pred_te\n",
    "\n",
    "    reg_fold = idx_r.iloc[te].copy()\n",
    "    reg_fold[\"y_true\"] = yr[te]\n",
    "    reg_fold[\"y_hat_p50\"] = pred_te\n",
    "    reg_fold[\"y_hat_naive\"] = y_naive_all[te]\n",
    "    reg_fold[\"fold\"] = k\n",
    "    reg_fold.to_csv(f\"{PRO}/reg_fold_p50_{k}.csv\", index=False)\n",
    "\n",
    "valid = (~np.isnan(oof_p50)) & (~np.isnan(yr)) & (~np.isnan(y_naive_all))\n",
    "y_true_eval   = yr[valid]\n",
    "y_model_eval  = oof_p50[valid]\n",
    "y_naive_eval  = y_naive_all[valid]\n",
    "\n",
    "mae_n, mae_n_lo, mae_n_hi = bootstrap_ci(mean_absolute_error, y_true_eval, y_naive_eval)\n",
    "sm_n,  sm_n_lo,  sm_n_hi  = bootstrap_ci(smape,              y_true_eval, y_naive_eval)\n",
    "\n",
    "mae_m, mae_m_lo, mae_m_hi = bootstrap_ci(mean_absolute_error, y_true_eval, y_model_eval)\n",
    "sm_m,  sm_m_lo,  sm_m_hi  = bootstrap_ci(smape,              y_true_eval, y_model_eval)\n",
    "\n",
    "reg_cmp = pd.DataFrame({\n",
    "    \"model\":        [\"Seasonal-Naive(63d)\", \"Quantile-LGB p50 (OOF)\"],\n",
    "    \"n_samples\":    [int(valid.sum())] * 2,\n",
    "    \"MAE\":          [mae_n,  mae_m],\n",
    "    \"MAE_lo95\":     [mae_n_lo, mae_m_lo],\n",
    "    \"MAE_hi95\":     [mae_n_hi, mae_m_hi],\n",
    "    \"SMAPE\":        [sm_n,   sm_m],\n",
    "    \"SMAPE_lo95\":   [sm_n_lo, sm_m_lo],\n",
    "    \"SMAPE_hi95\":   [sm_n_hi, sm_m_hi],\n",
    "}).round(6)\n",
    "\n",
    "reg_cmp_path = f\"{RPT}/regression_baseline_vs_model.csv\"\n",
    "reg_cmp.to_csv(reg_cmp_path, index=False)\n",
    "print(\"Saved →\", reg_cmp_path)\n",
    "reg_cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f61d4c-cfd9-4c88-ac91-c5d0cd64a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix, precision_recall_curve\n",
    ")\n",
    "\n",
    "def mean_pr_from_candidates(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            df = pd.read_csv(p)\n",
    "            if \"PR_AUC\" in df.columns and len(df):\n",
    "                return float(df[\"PR_AUC\"].mean())\n",
    "    return np.nan\n",
    "\n",
    "pr_logit = mean_pr_from_candidates([f\"{PRO}/cv_logistic.csv\", f\"{PRO}/cv_logistic_folds.csv\", f\"{PRO}/cv_logit_folds.csv\", f\"{PRO}/per_fold_eval_summary.csv\"])\n",
    "pr_xgb   = mean_pr_from_candidates([f\"{PRO}/cv_xgb.csv\",      f\"{PRO}/cv_xgb_folds.csv\",                               f\"{PRO}/per_fold_eval_summary.csv\"])\n",
    "pr_lgb   = mean_pr_from_candidates([f\"{PRO}/cv_lgb.csv\",      f\"{PRO}/cv_lgb_folds.csv\",                               f\"{PRO}/per_fold_eval_summary.csv\"])\n",
    "\n",
    "oof_path = f\"{PRO}/clf_oof_predictions.csv\"\n",
    "pr_ens = np.nan\n",
    "if os.path.exists(oof_path):\n",
    "    oof_df_tmp = pd.read_csv(oof_path, usecols=[\"y_true\",\"ens_tab\"])\n",
    "    m = oof_df_tmp.dropna(subset=[\"y_true\",\"ens_tab\"])\n",
    "    if len(m): pr_ens = float(average_precision_score(m[\"y_true\"].values, m[\"ens_tab\"].values))\n",
    "\n",
    "res = pd.DataFrame({\n",
    "    \"model\":  [\"Logistic\",\"XGBoost\",\"LightGBM\",\"Ensemble\"],\n",
    "    \"PR_AUC\": [pr_logit, pr_xgb,   pr_lgb,     pr_ens]\n",
    "})\n",
    "res.to_csv(f\"{PRO}/results_summary_classification.csv\", index=False)\n",
    "print(\"Saved →\", f\"{PRO}/results_summary_classification.csv\")\n",
    "display(res.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cb5fd3-bb4a-4c9a-a9cb-e0cb54f726e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_df = pd.read_csv(oof_path, parse_dates=[\"date\"])\n",
    "if \"fold\" not in oof_df.columns:\n",
    "    raise ValueError(\"`fold` column missing in OOF file.\")\n",
    "\n",
    "model_cols = [c for c in [\"logit\",\"xgb\",\"lgb\",\"ens_tab\"] if c in oof_df.columns]\n",
    "\n",
    "def best_threshold(y_true, scores):\n",
    "    prec, rec, th = precision_recall_curve(y_true, scores)\n",
    "    f1 = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "    idx = int(np.nanargmax(f1[:-1])) if len(f1) > 1 else 0\n",
    "    return (th[idx] if idx < len(th) else 0.5), float(prec[idx]), float(rec[idx]), float(f1[idx])\n",
    "\n",
    "rows = []\n",
    "for k in sorted(oof_df[\"fold\"].dropna().unique()):\n",
    "    part = oof_df.loc[oof_df[\"fold\"] == k].copy()\n",
    "    y_true_k = part[\"y_true\"].values.astype(int)\n",
    "    for m in model_cols:\n",
    "        s = part[m].values.astype(float)\n",
    "        valid = ~np.isnan(s)\n",
    "        if valid.sum() == 0: \n",
    "            continue\n",
    "        yv, sv = y_true_k[valid], s[valid]\n",
    "        pr_auc = average_precision_score(yv, sv)\n",
    "        th, p, r, f1 = best_threshold(yv, sv)\n",
    "        yhat = (sv >= th).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(yv, yhat).ravel()\n",
    "        cm_df = pd.DataFrame([[tn, fp],[fn, tp]],\n",
    "                             columns=[\"Pred 0\",\"Pred 1\"], index=[\"Actual 0\",\"Actual 1\"])\n",
    "        cm_df.to_csv(os.path.join(PRO, f\"cm_{m}_fold{k}.csv\"))\n",
    "        rows.append({\n",
    "            \"model\": m, \"fold\": int(k), \"n\": int(len(yv)),\n",
    "            \"PR_AUC\": float(pr_auc), \"threshold\": float(th),\n",
    "            \"Precision\": float(p), \"Recall\": float(r), \"F1\": float(f1),\n",
    "            \"TN\": int(tn), \"FP\": int(fp), \"FN\": int(fn), \"TP\": int(tp)\n",
    "        })\n",
    "\n",
    "if rows:\n",
    "    per_fold = pd.DataFrame(rows).sort_values([\"model\",\"fold\"]).reset_index(drop=True)\n",
    "    per_fold.to_csv(f\"{PRO}/per_fold_eval_summary.csv\", index=False)\n",
    "    avg = (per_fold\n",
    "           .groupby(\"model\")\n",
    "           .apply(lambda g: pd.Series({\n",
    "               \"folds\": int(g[\"fold\"].nunique()),\n",
    "               \"total_n\": int(g[\"n\"].sum()),\n",
    "               \"PR_AUC_mean\": float(np.average(g[\"PR_AUC\"], weights=g[\"n\"])),\n",
    "               \"Precision_mean\": float(np.average(g[\"Precision\"], weights=g[\"n\"])),\n",
    "               \"Recall_mean\": float(np.average(g[\"Recall\"], weights=g[\"n\"])),\n",
    "               \"F1_mean\": float(np.average(g[\"F1\"], weights=g[\"n\"]))\n",
    "           }))\n",
    "           .reset_index())\n",
    "    avg.to_csv(f\"{PRO}/per_fold_eval_averaged.csv\", index=False)\n",
    "    print(\"Saved →\", f\"{PRO}/per_fold_eval_summary.csv\")\n",
    "    print(\"Saved →\", f\"{PRO}/per_fold_eval_averaged.csv\")\n",
    "    display(avg.round(4))\n",
    "else:\n",
    "    print(\"No per-fold results produced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f20ba65-a6e0-4b77-8da9-b7f0b668cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "oof = pd.read_csv(f\"{PRO}/clf_oof_predictions.csv\")\n",
    "cv_logit = pd.read_csv(f\"{PRO}/cv_logistic_folds.csv\")\n",
    "cv_xgb   = pd.read_csv(f\"{PRO}/cv_xgb_folds.csv\")\n",
    "cv_lgb   = pd.read_csv(f\"{PRO}/cv_lgb_folds.csv\")\n",
    "\n",
    "w = {\n",
    "    \"logit\": cv_logit[\"PR_AUC\"].mean(),\n",
    "    \"xgb\":   cv_xgb[\"PR_AUC\"].mean(),\n",
    "    \"lgb\":   cv_lgb[\"PR_AUC\"].mean()\n",
    "}\n",
    "w_sum = sum(w.values())\n",
    "for k in w: w[k] /= w_sum\n",
    "\n",
    "oof[\"ens_weighted\"] = (\n",
    "    w[\"logit\"] * oof[\"logit\"] +\n",
    "    w[\"xgb\"]   * oof[\"xgb\"] +\n",
    "    w[\"lgb\"]   * oof[\"lgb\"]\n",
    ")\n",
    "\n",
    "y_true = oof[\"y_true\"].values\n",
    "scores = oof[\"ens_weighted\"].values\n",
    "prec, rec, th = precision_recall_curve(y_true, scores)\n",
    "\n",
    "candidates = [(t, P, R) for P, R, t in zip(prec, rec, th) if P>=0.70 and R>=0.30]\n",
    "if candidates:\n",
    "    best = sorted(candidates, key=lambda x: -2*x[1]*x[2]/(x[1]+x[2]+1e-12))[0]\n",
    "else:\n",
    "    f1 = 2*prec*rec/(prec+rec+1e-12)\n",
    "    idx = np.nanargmax(f1[:-1])\n",
    "    best = (th[idx], prec[idx], rec[idx])\n",
    "\n",
    "th_best, P_best, R_best = best[:3]\n",
    "pr_auc = average_precision_score(y_true, scores)\n",
    "print(f\"Weighted Ensemble PR_AUC={pr_auc:.3f} | τ={th_best:.3f} | P={P_best:.3f} | R={R_best:.3f}\")\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"model\":[\"WeightedEnsemble\"],\n",
    "    \"PR_AUC\":[pr_auc],\n",
    "    \"threshold\":[th_best],\n",
    "    \"precision\":[P_best],\n",
    "    \"recall\":[R_best]\n",
    "}).to_csv(f\"{PRO}/weighted_ensemble_metrics.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "for m in [\"logit\",\"xgb\",\"lgb\",\"ens_tab\",\"ens_weighted\"]:\n",
    "    if m not in oof.columns: continue\n",
    "    y, s = oof[\"y_true\"], oof[m]\n",
    "    Prec, Rec, _ = precision_recall_curve(y, s)\n",
    "    plt.plot(Rec, Prec, label=m)\n",
    "plt.title(\"Precision–Recall Curves (OOF)\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.legend(); plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{PRO}/pr_curve_models.png\", dpi=160)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "prob_true, prob_pred = calibration_curve(y_true, oof[\"ens_weighted\"], n_bins=15)\n",
    "plt.plot(prob_pred, prob_true, marker='o', label='Ensemble (weighted)')\n",
    "plt.plot([0,1],[0,1],'--',color='gray')\n",
    "plt.xlabel(\"Predicted probability\"); plt.ylabel(\"True positive rate\")\n",
    "plt.title(\"Calibration Curve\")\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{PRO}/calibration_plot.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "yhat = (oof[\"ens_weighted\"] >= th_best).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, yhat).ravel()\n",
    "print(f\"Confusion: TP={tp} FP={fp} TN={tn} FN={fn}\")\n",
    "\n",
    "err = oof.copy()\n",
    "err[\"pred_bin\"] = yhat\n",
    "err[\"error_type\"] = np.select(\n",
    "    [(y_true==1)&(yhat==0), (y_true==0)&(yhat==1)],\n",
    "    [\"False Negative\",\"False Positive\"],\n",
    "    default=\"Correct\"\n",
    ")\n",
    "sample_err = err.loc[err[\"error_type\"]!=\"Correct\"]\\\n",
    "    .sort_values(\"ens_weighted\", ascending=False)\\\n",
    "    .head(20)\n",
    "sample_err.to_csv(f\"{PRO}/error_ablation_top20.csv\", index=False)\n",
    "print(\"Saved representative FP/FN examples → error_ablation_top20.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
