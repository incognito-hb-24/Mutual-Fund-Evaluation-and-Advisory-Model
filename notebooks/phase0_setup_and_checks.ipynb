{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d07717d-4563-4a1b-be44-979ab9507935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment & Data Checks: Mount + Paths\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, sys, json, time, platform, subprocess, textwrap\n",
    "\n",
    "BASE = \"/content/drive/MyDrive/MLBA_Project\"\n",
    "RAW  = f\"{BASE}/data/raw\"\n",
    "PRO  = f\"{BASE}/data/processed\"\n",
    "RPT  = f\"{BASE}/reports\"\n",
    "MDIR = f\"{BASE}/models\"\n",
    "NBK  = f\"{BASE}/notebooks\"\n",
    "\n",
    "for d in (RAW, PRO, RPT, MDIR, NBK):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"Project root:\", BASE)\n",
    "print(\"Folders ready:\", {\"RAW\": RAW, \"PRO\": PRO, \"RPT\": RPT, \"MDIR\": MDIR, \"NBK\": NBK})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c3fe1-8421-48e6-8c06-e2277ea4981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System & package versions (helpful for reproducibility)\n",
    "\n",
    "import sys, platform, psutil, shutil\n",
    "from datetime import datetime\n",
    "\n",
    "def which(cmd):\n",
    "    p = shutil.which(cmd)\n",
    "    return p if p else \"not found\"\n",
    "\n",
    "info = {\n",
    "    \"timestamp_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"machine\": platform.machine(),\n",
    "    \"processor\": platform.processor(),\n",
    "    \"pip\": which(\"pip\"),\n",
    "    \"python_exe\": sys.executable\n",
    "}\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    info[\"cuda_available\"] = bool(torch.cuda.is_available())\n",
    "    info[\"torch_version\"] = torch.__version__\n",
    "except Exception:\n",
    "    info[\"cuda_available\"] = False\n",
    "    info[\"torch_version\"] = None\n",
    "\n",
    "try:\n",
    "    import psutil, os\n",
    "    vm = psutil.virtual_memory()\n",
    "    info[\"ram_gb\"] = round(vm.total / 1024**3, 2)\n",
    "except Exception:\n",
    "    info[\"ram_gb\"] = None\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "info[\"pandas_version\"] = pd.__version__\n",
    "info[\"numpy_version\"]  = np.__version__\n",
    "\n",
    "print(json.dumps(info, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d1550-6ed3-4bdc-967c-2cba7c58beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/ensure core pip packages (safe to re-run)\n",
    "# These match downstream phases.\n",
    "\n",
    "!pip -q install pandas numpy matplotlib scikit-learn lightgbm xgboost\n",
    "import pandas as pd, numpy as np, matplotlib, sklearn\n",
    "import lightgbm, xgboost\n",
    "\n",
    "print(\"OK →\",\n",
    "      \"pandas\", pd.__version__,\n",
    "      \"| numpy\", np.__version__,\n",
    "      \"| sklearn\", sklearn.__version__,\n",
    "      \"| lightgbm\", lightgbm.__version__,\n",
    "      \"| xgboost\", xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc0332-c70e-4ff6-9b52-84366de6ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a small project config for downstream notebooks to read if needed.\n",
    "\n",
    "cfg = {\n",
    "    \"base\": BASE,\n",
    "    \"paths\": {\"raw\": RAW, \"processed\": PRO, \"reports\": RPT, \"models\": MDIR, \"notebooks\": NBK},\n",
    "    \"horizon_days\": 63,\n",
    "    \"rf_annual\": 0.06,\n",
    "    \"random_seed\": 42,\n",
    "}\n",
    "\n",
    "with open(f\"{BASE}/project_config.json\", \"w\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "\n",
    "print(\"Wrote:\", f\"{BASE}/project_config.json\")\n",
    "print(json.dumps(cfg, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff747c-8c40-412f-9462-9d5859753cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify expected raw files exist (you can adjust names as needed).\n",
    "# This won’t fail the run; it prints actionable status.\n",
    "\n",
    "import os, glob\n",
    "\n",
    "expected_any_funds = \"*_LargeCap.csv\"         # pattern for funds\n",
    "expected_files = [\n",
    "    \"index_tri_nifty100.csv\",                 # required benchmark\n",
    "    # Optional macro inputs:\n",
    "    \"india_vix.csv\", \"usd_inr.csv\", \"gsec_10y.csv\", \"gold_inr.csv\", \"brent_crude.csv\"\n",
    "]\n",
    "\n",
    "print(\"RAW directory:\", RAW)\n",
    "present = os.listdir(RAW)\n",
    "print(\"RAW contains:\", len(present), \"files\")\n",
    "\n",
    "missing = []\n",
    "for fname in expected_files:\n",
    "    if not os.path.exists(os.path.join(RAW, fname)):\n",
    "        missing.append(fname)\n",
    "\n",
    "funds = sorted(glob.glob(os.path.join(RAW, expected_any_funds)))\n",
    "print(f\"Fund files matching '{expected_any_funds}':\", len(funds))\n",
    "\n",
    "if missing:\n",
    "    print(\"\\n⚠ Missing files (ok for optional macros; TRI is required for Phase 1):\")\n",
    "    for m in missing:\n",
    "        print(\"  -\", m)\n",
    "else:\n",
    "    print(\"\\nAll listed non-fund files present (or optional).\")\n",
    "\n",
    "if not funds:\n",
    "    print(\"\\n⚠ No fund CSVs found — Phase 1 will fail. Add *_LargeCap.csv files to RAW.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd38fc04-daa1-45cc-a13c-2f1631bb88a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek first two fund CSVs to ensure 'date' + NAV-like columns exist.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def sniff_schema(path):\n",
    "    df = pd.read_csv(path, nrows=5)\n",
    "    cols = [c.strip().lower() for c in df.columns]\n",
    "    has_date = \"date\" in cols\n",
    "    nav_candidates = [c for c in [\"nav\",\"net_asset_value\",\"nav_value\",\"price\",\"close\"] if c in cols]\n",
    "    return {\"file\": os.path.basename(path), \"has_date\": has_date, \"nav_cols\": nav_candidates, \"columns\": cols}\n",
    "\n",
    "sample = funds[:2]\n",
    "if not sample:\n",
    "    print(\"No sample available — add *_LargeCap.csv to RAW.\")\n",
    "else:\n",
    "    for p in sample:\n",
    "        print(sniff_schema(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc405df2-ef96-4e4a-96c6-5b3930963867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TRI timeline continuity and duplicates.\n",
    "\n",
    "import pandas as pd, numpy as np, os\n",
    "\n",
    "tri_path = os.path.join(RAW, \"index_tri_nifty100.csv\")\n",
    "if not os.path.exists(tri_path):\n",
    "    print(\"⚠ TRI file missing — required for Phase 1.\")\n",
    "else:\n",
    "    tri = pd.read_csv(tri_path)\n",
    "    tri.columns = tri.columns.str.strip().str.lower()\n",
    "    assert \"date\" in tri.columns, \"TRI must have a 'date' column\"\n",
    "    tri[\"date\"] = pd.to_datetime(tri[\"date\"], errors=\"coerce\")\n",
    "    tri = tri.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "    val_col = [c for c in tri.columns if c != \"date\"][0]\n",
    "    tri[\"dupe\"] = tri.duplicated(subset=[\"date\"])\n",
    "    print(\"TRI range:\", tri[\"date\"].min().date(), \"→\", tri[\"date\"].max().date(),\n",
    "          \"| rows:\", len(tri), \"| duplicates:\", int(tri[\"dupe\"].sum()))\n",
    "    if tri[\"dupe\"].any():\n",
    "        display(tri.loc[tri[\"dupe\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791881e4-07f0-40a7-a0c0-405eeb5626c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders so downstream phases don’t crash if run before Phase 1/2.\n",
    "# These are overwritten by actual outputs later.\n",
    "\n",
    "import pandas as pd, numpy as np, os, json\n",
    "\n",
    "place_master = os.path.join(PRO, \"clean_master_union.csv\")\n",
    "place_feat   = os.path.join(PRO, \"clean_with_features.csv\")\n",
    "\n",
    "if not os.path.exists(place_master):\n",
    "    pd.DataFrame(columns=[\"date\",\"fund_id\",\"nav\",\"tri\"]).to_csv(place_master, index=False)\n",
    "    print(\"Scaffolded:\", place_master)\n",
    "\n",
    "if not os.path.exists(place_feat):\n",
    "    pd.DataFrame(columns=[\"date\",\"fund_id\"]).to_csv(place_feat, index=False)\n",
    "    print(\"Scaffolded:\", place_feat)\n",
    "\n",
    "open(os.path.join(PRO, \"README.txt\"), \"w\").write(\n",
    "    \"Processed artifacts are generated by notebooks.\\n\"\n",
    ")\n",
    "print(\"Processed scaffolding ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8918d53-3923-4d11-8d47-29b95654d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: create a data_dictionary.json you can edit with column descriptions.\n",
    "\n",
    "dd = {\n",
    "  \"fund_files_pattern\": \"*_LargeCap.csv\",\n",
    "  \"fund_schema\": {\n",
    "    \"date\": \"Trading date (YYYY-MM-DD)\",\n",
    "    \"nav|net_asset_value|nav_value|price|close\": \"Mutual fund NAV / price column (one of these)\"\n",
    "  },\n",
    "  \"index_tri_nifty100.csv\": {\n",
    "    \"date\": \"Trading date\",\n",
    "    \"tri\": \"Total Return Index level (benchmark)\"\n",
    "  },\n",
    "  \"india_vix.csv\": {\"date\": \"Trading date\", \"india_vix\": \"Volatility index\"},\n",
    "  \"usd_inr.csv\": {\"date\": \"Trading date\", \"usd_inr\": \"USD/INR exchange rate\"},\n",
    "  \"gsec_10y.csv\": {\"date\": \"Trading date\", \"gsec_10y_yield\": \"10Y government bond yield\"},\n",
    "  \"gold_inr.csv\": {\"date\": \"Trading date\", \"gold_inr\": \"Gold (INR) index/price\"},\n",
    "  \"brent_crude.csv\": {\"date\": \"Trading date\", \"brent_usd\": \"Brent crude price (USD)\"}\n",
    "}\n",
    "\n",
    "out = os.path.join(RAW, \"data_dictionary.json\")\n",
    "with open(out, \"w\") as f:\n",
    "    json.dump(dd, f, indent=2)\n",
    "\n",
    "print(\"Wrote:\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df938679-7bc5-4d22-bf57-a003fdc61ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable integrity helpers (import these via %run if you want in later phases)\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "def assert_no_duplicate_keys(df, keys):\n",
    "    dups = df.duplicated(subset=keys).sum()\n",
    "    if dups:\n",
    "        raise AssertionError(f\"Found {dups} duplicate rows on keys={keys}\")\n",
    "\n",
    "def assert_monotonic_by_group(df, group_col, date_col):\n",
    "    bad = []\n",
    "    for gid, g in df.groupby(group_col):\n",
    "        if not g[date_col].is_monotonic_increasing:\n",
    "            bad.append(gid)\n",
    "    if bad:\n",
    "        raise AssertionError(f\"Non-monotonic dates for groups: {bad[:5]}{'...' if len(bad)>5 else ''}\")\n",
    "\n",
    "def pct_missing(s):\n",
    "    return round(float(s.isna().mean())*100, 2)\n",
    "\n",
    "print(\"Helpers ready:\", [\"assert_no_duplicate_keys\", \"assert_monotonic_by_group\", \"pct_missing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e8af6-03b9-43b0-9da5-a0da56b68cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark Phase 0 completed (downstream notebooks can optionally check this)\n",
    "\n",
    "stamp = {\n",
    "    \"phase\": 0,\n",
    "    \"message\": \"Environment initialized & raw checks executed\",\n",
    "    \"time_utc\": __import__(\"datetime\").datetime.utcnow().isoformat() + \"Z\"\n",
    "}\n",
    "with open(f\"{PRO}/phase0_stamp.json\", \"w\") as f:\n",
    "    json.dump(stamp, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", f\"{PRO}/phase0_stamp.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
