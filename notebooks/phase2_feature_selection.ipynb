{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b8b45-a64f-4af2-b566-b8b3d87ee505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT: Load the union master; verify schema and coverage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, pandas as pd, numpy as np\n",
    "BASE = \"/content/drive/MyDrive/MLBA_Project\"\n",
    "RAW  = f\"{BASE}/data/raw\"\n",
    "PRO  = f\"{BASE}/data/processed\"\n",
    "os.makedirs(PRO, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(f\"{PRO}/clean_master_union.csv\", parse_dates=[\"date\"])\n",
    "df.columns = df.columns.str.lower()\n",
    "df = df.sort_values([\"fund_id\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "req = {\"date\",\"fund_id\",\"nav\",\"tri\",\"india_vix\",\"usd_inr\",\"gsec_10y_yield\",\"gold_inr\",\"brent_usd\"}\n",
    "missing = [c for c in req if c not in df.columns]\n",
    "print(\"Rows:\", len(df), \"| funds:\", df[\"fund_id\"].nunique(),\n",
    "      \"| range:\", df[\"date\"].min().date(), \"→\", df[\"date\"].max().date())\n",
    "print(\"Missing (ok if some macros were not provided):\", missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fd1308-5a80-4347-9ac5-afa393eeb058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT: Daily % returns and multi-horizon returns/excess\n",
    "df[\"ret_1d\"] = df.groupby(\"fund_id\")[\"nav\"].pct_change()\n",
    "df[\"bmk_1d\"] = df[\"tri\"].pct_change()\n",
    "\n",
    "horizons = [7,14,21,63]\n",
    "for h in horizons:\n",
    "    df[f\"ret_{h}d\"] = df.groupby(\"fund_id\")[\"nav\"].pct_change(h)\n",
    "    df[f\"bmk_{h}d\"] = df[\"tri\"].pct_change(h)\n",
    "    df[f\"excess_{h}d\"] = df[f\"ret_{h}d\"] - df[f\"bmk_{h}d\"]\n",
    "\n",
    "print(\"Added daily & multi-horizon momentum/excess.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77048c56-c2f2-4159-9fd3-20c98772a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT: Rolling volatility & Sharpe (126d)\n",
    "import numpy as np\n",
    "rf_annual = 0.06\n",
    "rf_daily  = rf_annual / 252.0\n",
    "win = 126\n",
    "\n",
    "roll_std  = df.groupby(\"fund_id\")[\"ret_1d\"].rolling(win).std().reset_index(level=0, drop=True)\n",
    "roll_mean = df.groupby(\"fund_id\")[\"ret_1d\"].rolling(win).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "df[\"vol_126d\"]    = roll_std * np.sqrt(252)\n",
    "den = roll_std.replace(0, np.nan)\n",
    "df[\"sharpe_126d\"] = (roll_mean - rf_daily) / den\n",
    "print(\"Added vol_126d, sharpe_126d.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df00aa9e-242d-4839-acd9-b02790b700f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT: Rolling beta vs TRI and annualized alpha (126d)\n",
    "win_beta = 126\n",
    "def beta_alpha_block(g):\n",
    "    r = g[\"ret_1d\"]; m = g[\"bmk_1d\"]\n",
    "    cov = r.rolling(win_beta).cov(m)\n",
    "    var = m.rolling(win_beta).var()\n",
    "    beta = cov / var.replace(0, np.nan)\n",
    "    mean_r = r.rolling(win_beta).mean()\n",
    "    mean_m = m.rolling(win_beta).mean()\n",
    "    alpha_daily = (mean_r - rf_daily) - beta * (mean_m - rf_daily)\n",
    "    alpha_ann = alpha_daily * 252\n",
    "    return pd.DataFrame({\"beta_126d\": beta, \"alpha_ann_126d\": alpha_ann})\n",
    "\n",
    "ba = df.groupby(\"fund_id\", group_keys=False).apply(beta_alpha_block)\n",
    "df[[\"beta_126d\",\"alpha_ann_126d\"]] = ba.values\n",
    "print(\"Added beta_126d, alpha_ann_126d.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdff849-a1e7-408d-82e0-5f2e8896e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT: Rolling 1y max drawdown and % positive days over 63d\n",
    "win_dd, win_cons = 252, 63\n",
    "\n",
    "roll_peak = (df.groupby(\"fund_id\")[\"nav\"]\n",
    "               .rolling(win_dd, min_periods=1).max()\n",
    "               .reset_index(level=0, drop=True))\n",
    "dd = df[\"nav\"] / roll_peak - 1.0\n",
    "\n",
    "mdd = (dd.groupby(df[\"fund_id\"])\n",
    "         .rolling(win_dd, min_periods=1).min()\n",
    "         .reset_index(level=0, drop=True))\n",
    "df[\"mdd_252d\"] = mdd.values\n",
    "\n",
    "is_pos = (df[\"ret_1d\"] > 0).astype(float)\n",
    "cons = (is_pos.groupby(df[\"fund_id\"])\n",
    "              .rolling(win_cons, min_periods=1).mean()\n",
    "              .reset_index(level=0, drop=True))\n",
    "df[\"consistency_pos_63d\"] = cons.values\n",
    "\n",
    "print(\"Added mdd_252d, consistency_pos_63d.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c716d2-4e6d-405f-b450-eb0b370d655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT: Rolling correlation with benchmark and simple regime flags (63d)\n",
    "win_reg = 63\n",
    "def rolling_corr(g):\n",
    "    return g[\"ret_1d\"].rolling(win_reg).corr(g[\"bmk_1d\"])\n",
    "\n",
    "df[\"corr_bmk_63d\"] = df.groupby(\"fund_id\", group_keys=False).apply(rolling_corr).values\n",
    "df[\"regime_bull_63d\"] = (df[\"bmk_63d\"] > 0).astype(int)\n",
    "df[\"regime_bear_63d\"] = (df[\"bmk_63d\"] < 0).astype(int)\n",
    "\n",
    "print(\"Added corr_bmk_63d + regime flags.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3cf326-b973-4aac-85e0-dcb50613384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT: Lag macro factors by 5/10 business days\n",
    "macro_cols = [c for c in [\"india_vix\",\"usd_inr\",\"gsec_10y_yield\",\"gold_inr\",\"brent_usd\"] if c in df.columns]\n",
    "for c in macro_cols:\n",
    "    df[f\"{c}_lag5\"]  = df[c].shift(5)\n",
    "    df[f\"{c}_lag10\"] = df[c].shift(10)\n",
    "print(\"Added macro lags for:\", macro_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe12c6e-f474-444d-85a3-fb9cd39b58a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT: Cross-sectional ranks per date\n",
    "def xs_rank(col, asc=False):\n",
    "    return df.groupby(\"date\")[col].rank(pct=True, ascending=asc)\n",
    "\n",
    "if \"excess_21d\" in df.columns:\n",
    "    df[\"xs_rank_excess_21d\"]  = xs_rank(\"excess_21d\", asc=False)\n",
    "if \"sharpe_126d\" in df.columns:\n",
    "    df[\"xs_rank_sharpe_126d\"] = xs_rank(\"sharpe_126d\", asc=False)\n",
    "if \"mdd_252d\" in df.columns:\n",
    "    df[\"xs_rank_mdd_252d\"]    = xs_rank(\"mdd_252d\", asc=True)\n",
    "\n",
    "print(\"Added cross-section ranks (if available).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c857db-988c-4794-b898-cf10355e8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT: Drop sparse columns (>=80% NaN) and sparse rows (<90% filled)\n",
    "col_keep = [c for c in df.columns if c not in [\"fund_id\",\"date\"]]\n",
    "col_na = df[col_keep].isna().mean()\n",
    "cols_ok = [\"fund_id\",\"date\"] + [c for c in col_keep if col_na[c] < 0.80]\n",
    "\n",
    "df_feat = df[cols_ok].copy()\n",
    "row_nonmiss = 1.0 - df_feat.isna().mean(axis=1)\n",
    "df_feat = df_feat[row_nonmiss >= 0.90].copy()\n",
    "df_feat = df_feat.sort_values([\"fund_id\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "out_features = f\"{PRO}/clean_with_features.csv\"\n",
    "df_feat.to_csv(out_features, index=False)\n",
    "print(\"Saved features →\", out_features, \"| rows:\", len(df_feat), \"| cols:\", len(df_feat.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d1e21a-6253-439b-a158-539e202d3c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_of = df_feat[\"date\"].max()\n",
    "latest = df_feat[df_feat[\"date\"] == as_of].copy()\n",
    "\n",
    "def safe_rank(s, asc=False): \n",
    "    return s.rank(pct=True, ascending=asc)\n",
    "\n",
    "parts = []\n",
    "if \"excess_63d\" in latest.columns:   parts.append(0.6 * safe_rank(latest[\"excess_63d\"], asc=False))\n",
    "if \"excess_21d\" in latest.columns:   parts.append(0.2 * safe_rank(latest[\"excess_21d\"], asc=False))\n",
    "if \"sharpe_126d\" in latest.columns:  parts.append(0.2 * safe_rank(latest[\"sharpe_126d\"], asc=False))\n",
    "\n",
    "if parts:\n",
    "    latest[\"score\"] = sum(parts)\n",
    "    cols = [\"fund_id\",\"date\",\"score\"] + [c for c in [\"excess_63d\",\"excess_21d\",\"sharpe_126d\",\"vol_126d\",\"mdd_252d\"] if c in latest.columns]\n",
    "    leaderboard = latest[cols].sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "else:\n",
    "    leaderboard = latest[[\"fund_id\",\"date\"]].copy()\n",
    "    leaderboard[\"score\"] = np.nan\n",
    "\n",
    "leader_path = f\"{PRO}/leaderboard.csv\"\n",
    "leaderboard.to_csv(leader_path, index=False)\n",
    "print(\"Saved leaderboard →\", leader_path, \"| rows:\", len(leaderboard), \"| as-of:\", as_of.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73466a90-80a2-41d6-ab47-62c5cb3547ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [{\"column\": c, \"dtype\": str(df_feat[c].dtype), \"missing_pct\": float(df_feat[c].isna().mean())} \n",
    "          for c in df_feat.columns]\n",
    "dc = pd.DataFrame(schema).sort_values(\"missing_pct\", ascending=False)\n",
    "dc.to_csv(f\"{PRO}/data_card.csv\", index=False)\n",
    "dc.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
